{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "cross-validating...\n",
      "# fold 1, Sun Oct 23 20:32:02 2016\n",
      "# AUC: 84.94%\n",
      "\n",
      "# fold 2, Sun Oct 23 20:32:49 2016\n",
      "# AUC: 84.70%\n",
      "\n",
      "# fold 3, Sun Oct 23 20:33:34 2016\n",
      "# AUC: 84.35%\n",
      "\n",
      "# fold 4, Sun Oct 23 20:34:22 2016\n",
      "# AUC: 84.29%\n",
      "\n",
      "# fold 5, Sun Oct 23 20:35:07 2016\n",
      "# AUC: 84.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cross_validation as CV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "from time import ctime\n",
    "\n",
    "#\n",
    "\n",
    "train_file = '~/Documents/repos/chimera/notebooks/00000003/data/train.csv'\n",
    "test_file = '~/Documents/repos/chimera/notebooks/00000003/data//test.csv'\n",
    "output_file = '~/Documents/repos/chimera/notebooks/00000003/data/train_sorted.csv'\n",
    "\n",
    "print \"loading...\"\n",
    "\n",
    "train = pd.read_csv( train_file )\n",
    "test = pd.read_csv( test_file )\n",
    "\n",
    "test.drop( 't_id', axis = 1, inplace = True )\n",
    "test['target'] = 0 # dummy for preserving column order when concatenating\n",
    "\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "\n",
    "orig_train = train.copy()\n",
    "assert( np.all( orig_train.columns == test.columns ))\n",
    "\n",
    "train = pd.concat(( orig_train, test ))\n",
    "train.reset_index( inplace = True, drop = True )\n",
    "\n",
    "x = train.drop( [ 'is_test', 'target' ], axis = 1 )\n",
    "y = train.is_test\n",
    "\n",
    "#\n",
    "\n",
    "print \"cross-validating...\"\n",
    "\n",
    "n_estimators = 100\n",
    "clf = RF( n_estimators = n_estimators, n_jobs = -1 )\n",
    "\n",
    "predictions = np.zeros( y.shape )\n",
    "\n",
    "cv = CV.StratifiedKFold( y, n_folds = 5, shuffle = True, random_state = 5678 )\n",
    "\n",
    "for f, ( train_i, test_i ) in enumerate( cv ):\n",
    "\n",
    "    print \"# fold {}, {}\".format( f + 1, ctime())\n",
    "\n",
    "    x_train = x.iloc[train_i]\n",
    "    x_test = x.iloc[test_i]\n",
    "    y_train = y.iloc[train_i]\n",
    "    y_test = y.iloc[test_i]\n",
    "\n",
    "    clf.fit( x_train, y_train )\t\n",
    "\n",
    "    p = clf.predict_proba( x_test )[:,1]\n",
    "\n",
    "    auc = AUC( y_test, p )\n",
    "    print \"# AUC: {:.2%}\\n\".format( auc )\n",
    "\n",
    "    predictions[ test_i ] = p\n",
    "\n",
    "# fold 1\n",
    "# AUC: 87.00%\n",
    "\n",
    "# fold 2\n",
    "# AUC: 86.87%\n",
    "\n",
    "# fold 3\n",
    "# AUC: 87.43%\n",
    "\n",
    "# fold 4\n",
    "# AUC: 86.83%\n",
    "\n",
    "# fold 5\n",
    "# AUC: 87.71%\n",
    "\n",
    "train['p'] = predictions\n",
    "\n",
    "i = predictions.argsort()\n",
    "train_sorted = train.iloc[i]\n",
    "\n",
    "# \"\"\"\n",
    "# print \"predictions distribution for test\"\n",
    "# train_sorted.loc[ train_sorted.is_test == 1, 'p' ].hist()\n",
    "# p_test_mean = train_sorted.loc[ train_sorted.is_test == 1, 'p' ].mean()\n",
    "# p_test_std = train_sorted.loc[ train_sorted.is_test == 1, 'p' ].std()\n",
    "# print \"# mean: {}, std: {}\".format( p_test_mean, p_test_std )\n",
    "# # mean: 0.404749669062, std: 0.109116404564\n",
    "# \"\"\"\n",
    "\n",
    "train_sorted = train_sorted.loc[ train_sorted.is_test == 0 ]\n",
    "assert( train_sorted.target.sum() == orig_train.target.sum())\n",
    "\n",
    "# \"\"\"\n",
    "# print \"predictions distribution for train\"\n",
    "# p_train_mean = train_sorted.p.mean()\n",
    "# p_train_std = train_sorted.p.std()\n",
    "# print \"# mean: {}, std: {}\".format( p_train_mean, p_train_std )\n",
    "# # mean: 0.293768613822, std: 0.113601453932\n",
    "# \"\"\"\n",
    "\n",
    "train_sorted.drop( 'is_test', axis = 1, inplace = True )\n",
    "train_sorted.to_csv( output_file, index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "No transformation\n",
      "AUC: 52.33%, accuracy: 51.44%, log loss: 69.26% \n",
      "\n",
      "MaxAbsScaler(copy=True)\n",
      "AUC: 52.33%, accuracy: 51.46%, log loss: 69.26% \n",
      "\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "AUC: 52.33%, accuracy: 51.48%, log loss: 69.26% \n",
      "\n",
      "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "       with_scaling=True)\n",
      "AUC: 52.33%, accuracy: 51.44%, log loss: 69.26% \n",
      "\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "AUC: 52.33%, accuracy: 51.40%, log loss: 69.26% \n",
      "\n",
      "Normalizer(copy=True, norm='l1')\n",
      "AUC: 51.84%, accuracy: 51.00%, log loss: 69.32% \n",
      "\n",
      "Normalizer(copy=True, norm='l2')\n",
      "AUC: 52.12%, accuracy: 51.18%, log loss: 69.32% \n",
      "\n",
      "Normalizer(copy=True, norm='max')\n",
      "AUC: 52.35%, accuracy: 51.38%, log loss: 69.27% \n",
      "\n",
      "Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)))])\n",
      "AUC: 51.75%, accuracy: 50.78%, log loss: 69.52% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"Load sorted training set and validate on examples looking the most like test\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#\n",
    "\n",
    "input_file = '~/Documents/repos/chimera/notebooks/00000003/data/train_sorted.csv'\n",
    "val_size = 5000\n",
    "\n",
    "#\n",
    "\n",
    "def train_and_evaluate( y_train, x_train, y_val, x_val ):\n",
    "\n",
    "    lr = LR()\n",
    "    lr.fit( x_train, y_train )\n",
    "\n",
    "    p = lr.predict_proba( x_val )\n",
    "    p_bin = lr.predict( x_val )\n",
    "\n",
    "    acc = accuracy( y_val, p_bin )\n",
    "    auc = AUC( y_val, p[:,1] )\n",
    "    ll = log_loss( y_val, p[:,1] )\n",
    "\n",
    "    return ( auc, acc, ll )\n",
    "\n",
    "def transform_train_and_evaluate( transformer ):\n",
    "\n",
    "    global x_train, x_val, y_train\n",
    "\n",
    "    x_train_new = transformer.fit_transform( x_train )\n",
    "    x_val_new = transformer.transform( x_val )\n",
    "\n",
    "    return train_and_evaluate( y_train, x_train_new, y_val, x_val_new )\n",
    "\n",
    "#\n",
    "\n",
    "print \"loading...\"\n",
    "\n",
    "data = pd.read_csv( input_file )\n",
    "\n",
    "train = data.iloc[:-val_size]\n",
    "val = data.iloc[-val_size:]\n",
    "\n",
    "# print len( train ), len( val )\n",
    "\n",
    "# \n",
    "\n",
    "y_train = train.target.values\n",
    "y_val = val.target.values\n",
    "\n",
    "x_train = train.drop( 'target', axis = 1 )\n",
    "x_val = val.drop( 'target', axis = 1 )\n",
    "\n",
    "# train, predict, evaluate\n",
    "\n",
    "auc, acc, ll = train_and_evaluate( y_train, x_train, y_val, x_val )\n",
    "\n",
    "print \"No transformation\"\n",
    "print \"AUC: {:.2%}, accuracy: {:.2%}, log loss: {:.2%} \\n\".format( auc, acc, ll )\n",
    "\n",
    "# try different transformations for X\n",
    "\n",
    "transformers = [ MaxAbsScaler(), MinMaxScaler(), RobustScaler(), StandardScaler(),  \n",
    "    Normalizer( norm = 'l1' ), Normalizer( norm = 'l2' ), Normalizer( norm = 'max' ) ]\n",
    "\n",
    "poly_scaled = Pipeline([ ( 'poly', PolynomialFeatures()), ( 'scaler', MinMaxScaler()) ])\n",
    "\n",
    "transformers += [ poly_scaled ]\n",
    "\n",
    "for transformer in transformers:\n",
    "\n",
    "    print transformer\n",
    "    auc, acc, ll = transform_train_and_evaluate( transformer )\n",
    "    print \"AUC: {:.2%}, accuracy: {:.2%}, log loss: {:.2%} \\n\".format( auc, acc, ll )\n",
    "\n",
    "# \"\"\"\n",
    "# No transformation\n",
    "# AUC: 52.54%, accuracy: 51.96%, log loss: 69.22%\n",
    "# MaxAbsScaler(copy=True)\n",
    "# AUC: 52.54%, accuracy: 51.98%, log loss: 69.22%\n",
    "# MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "# AUC: 52.54%, accuracy: 51.98%, log loss: 69.22%\n",
    "# RobustScaler(copy=True, with_centering=True, with_scaling=True)\n",
    "# AUC: 52.54%, accuracy: 52.04%, log loss: 69.22%\n",
    "# StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# AUC: 52.53%, accuracy: 52.04%, log loss: 69.22%\n",
    "# Normalizer(copy=True, norm='l1')\n",
    "# AUC: 52.30%, accuracy: 52.46%, log loss: 69.23%\n",
    "# Normalizer(copy=True, norm='l2')\n",
    "# AUC: 52.35%, accuracy: 51.08%, log loss: 69.24%\n",
    "# Normalizer(copy=True, norm='max')\n",
    "# AUC: 52.37%, accuracy: 52.20%, log loss: 69.24%\n",
    "# Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=\n",
    "# False)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)))])\n",
    "# AUC: 52.57%, accuracy: 51.76%, log loss: 69.58%\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "training...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "predicting...\n",
      "saving...\n"
     ]
    }
   ],
   "source": [
    "# \"Load data, train, output predictions\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "train_file = '~/Documents/repos/chimera/notebooks/00000003/data/train.csv'\n",
    "test_file = '~/Documents/repos/chimera/notebooks/00000003/data/test.csv'\n",
    "lr_output_file = '~/Documents/repos/chimera/notebooks/00000003/data/predictions_lr.csv'\n",
    "poly_output_file = '~/Documents/repos/chimera/notebooks/00000003/data/predictions_poly.csv'\n",
    "\n",
    "#\n",
    "\n",
    "print \"loading...\"\n",
    "\n",
    "train = pd.read_csv( train_file )\n",
    "test = pd.read_csv( test_file )\n",
    "\n",
    "x_train = train.drop( 'target', axis = 1 )\n",
    "y_train = train.target.values\n",
    "\n",
    "x_test = test.drop( 't_id', axis = 1 )\n",
    "\n",
    "print \"training...\"\n",
    "\n",
    "lr = LR()\n",
    "print lr\n",
    "lr.fit( x_train, y_train )\n",
    "\n",
    "poly = make_pipeline( PolynomialFeatures(), LR()) \n",
    "print poly\n",
    "poly.fit( x_train, y_train )\n",
    "\n",
    "print \"predicting...\"\n",
    "\n",
    "p_lr = lr.predict_proba( x_test )\n",
    "test['p_lr'] = p_lr[:,1]\n",
    "\n",
    "p_poly = poly.predict_proba( x_test )\n",
    "test['p_poly'] = p_poly[:,1]\n",
    "\n",
    "print \"saving...\"\n",
    "\n",
    "test.to_csv( lr_output_file, columns = ( 't_id', 'p_lr' ), header = ( 't_id', 'probability' ), index = None )\n",
    "test.to_csv( poly_output_file, columns = ( 't_id', 'p_poly' ), header = ( 't_id', 'probability' ), index = None )\n",
    "\n",
    "# LR:\t0.69101\n",
    "# Poly:\t0.69229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
